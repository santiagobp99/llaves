{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e26838b-4cc8-4743-a4e6-92106e4022c8",
   "metadata": {},
   "source": [
    "# Despliegue en Flask de la deteccion multirostro de deepfakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfe11bcc-7bdf-4465-b86b-1d275a189df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras_vggface\n",
      "  Using cached keras_vggface-0.6-py3-none-any.whl (8.3 kB)\n",
      "Requirement already satisfied: pillow in c:\\users\\hp\\anaconda3\\lib\\site-packages (from keras_vggface) (9.0.1)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\hp\\anaconda3\\lib\\site-packages (from keras_vggface) (6.0)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from keras_vggface) (1.7.3)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from keras_vggface) (1.21.5)\n",
      "Requirement already satisfied: keras in c:\\users\\hp\\anaconda3\\lib\\site-packages (from keras_vggface) (2.9.0)\n",
      "Requirement already satisfied: h5py in c:\\users\\hp\\anaconda3\\lib\\site-packages (from keras_vggface) (3.6.0)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from keras_vggface) (1.16.0)\n",
      "Installing collected packages: keras-vggface\n",
      "Successfully installed keras-vggface-0.6\n"
     ]
    }
   ],
   "source": [
    "!pip install keras_vggface\n",
    "! pip install git+https://github.com/rcmalli/keras-vggface.git\n",
    "!pip install keras_applications --no-deps\n",
    "filename = 'C:/Users/HP/anaconda3/Lib/site-packages/keras_vggface/models.py'\n",
    "text = open(filename).read()\n",
    "open(filename, \"w+\").write(text.replace('keras.engine.topology', 'tensorflow.keras.utils'))\n",
    "!pip install mtcnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da5cde77-e8ee-475a-9869-df9234143f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import time\n",
    "import _pickle as pickle\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import random\n",
    "from IPython.display import Video\n",
    "import cv2 \n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from numpy import asarray\n",
    "from scipy.spatial.distance import cosine\n",
    "import sys\n",
    "import statistics\n",
    "import pickle\n",
    "from statistics import mode\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "from keras_vggface.vggface import VGGFace\n",
    "from keras_vggface.utils import preprocess_input as pre\n",
    "\n",
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from keras.models import Model, load_model\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, make_scorer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.layers import Input\n",
    "\n",
    "from google.cloud import storage\n",
    "from google.cloud.storage import blob\n",
    "import keras\n",
    "\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "from flask import Flask, request, jsonify, url_for, render_template, flash, redirect\n",
    "import uuid\n",
    "import os\n",
    "import numpy as np\n",
    "from werkzeug.utils import secure_filename\n",
    "from tensorflow.keras.applications import MobileNet\n",
    "from PIL import Image, ImageFile\n",
    "from io import BytesIO\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.mobilenet import preprocess_input\n",
    "from tensorflow.keras.applications.mobilenet import decode_predictions\n",
    "#from app import app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c884397-fecb-4b9a-9462-b92f8f63f708",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = ['FAKE', 'REAL']\n",
    "IMG_SIZE = 299\n",
    "def build_feature_extractor():\n",
    "    feature_extractor = InceptionV3(\n",
    "        weights=\"imagenet\",\n",
    "        include_top=False,\n",
    "        pooling=\"avg\",\n",
    "        input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "    )\n",
    "    preprocess_input = keras.applications.inception_v3.preprocess_input\n",
    "\n",
    "    inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))\n",
    "    preprocessed = preprocess_input(inputs)\n",
    "\n",
    "    outputs = feature_extractor(preprocessed)\n",
    "    return keras.Model(inputs, outputs, name=\"feature_extractor\")\n",
    "\n",
    "\n",
    "feature_extractor = build_feature_extractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e2dba716-bdcb-4f72-9336-fbe4dee98b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "# original\n",
    "def extract_face(frame,detector,required_size=(299, 299)):\n",
    "        # create the detector, using default wei        ghts\n",
    "        # detect faces in the image\n",
    "        faces = detector.detect_faces(frame)\n",
    "        # extract the bounding box from the first face\n",
    "        face_images = []\n",
    "        arr =[]\n",
    "        for face in faces:\n",
    "            if face[\"confidence\"] > 0.96:\n",
    "                arr.append(face[\"confidence\"])\n",
    "                # extract the bounding box from the requested face\n",
    "                x1, y1, width, height = face['box']\n",
    "                x2, y2 = x1 + width, y1 + height\n",
    "                \n",
    "                # extract the face\n",
    "                face_boundary = frame[y1:y2, x1:x2]\n",
    "\n",
    "                # resize pixels to the model size\n",
    "                face_image = Image.fromarray(face_boundary)\n",
    "                face_image = face_image.resize(required_size)\n",
    "                face_array = np.asarray(face_image)\n",
    "                face_images.append(face_array)\n",
    "\n",
    "        return face_images\n",
    "\n",
    "def get_embedding(face,model):\n",
    "    # convert into an array of samples\n",
    "    sample = [np.asarray(face, 'float32')]\n",
    "    # prepare the face for the model, e.g. center pixels\n",
    "    sample = pre(sample, version=2)\n",
    "    # perform prediction\n",
    "    yhat = model.predict(sample)    \n",
    "    return yhat       \n",
    "\n",
    "\n",
    "def is_match(i,j,a,b,show_faces,n_faces,embedings,thresh=0.4):\n",
    "    # calculate distance between embeddings\n",
    "    score = cosine(embedings[i][j], embedings[a][b])\n",
    "    if show_faces==True:\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "        ax1.imshow(n_faces[i][j])\n",
    "        ax1.set_title('ID face')\n",
    "        ax2.imshow(n_faces[a][b])\n",
    "        ax2.set_title('Subject face')\n",
    "\n",
    "    return score\n",
    "\n",
    "detector = MTCNN()\n",
    "model = VGGFace(model='resnet50', include_top=False, input_shape=(299, 299, 3), pooling='avg')\n",
    "\n",
    "def nueva(path):\n",
    "    sys.stdout = open(os.devnull, 'w')\n",
    "    video = \"static/uploads/\" + path\n",
    "    print(path)\n",
    "    v_cap = cv2.VideoCapture(video)\n",
    "    v_len = int(v_cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    # Loop through video, taking a handful of frames to form a batch\n",
    "    faces = []\n",
    "    modas = {}\n",
    "    moda = 0\n",
    "    for i in range(v_len):\n",
    "        # Load face\n",
    "        success = v_cap.grab()\n",
    "        if moda < 50:\n",
    "            success, frame = v_cap.retrieve()\n",
    "            if success:\n",
    "                cara = extract_face(cv2.cvtColor(frame,cv2.COLOR_BGR2RGB),detector)\n",
    "                if cara != []:\n",
    "                    tam = len(cara)\n",
    "                    modas[tam] = modas.get(tam, 0) +1\n",
    "                    faces.append(cara)\n",
    "                    moda = max(moda, modas[tam])\n",
    "                else:\n",
    "                    continue\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "    caras = [len(x) for x in faces]\n",
    "    mode = statistics.mode(caras)\n",
    "    n_faces = [x for x in faces if len(x) == mode]\n",
    "\n",
    "    emb = [[get_embedding(y,model) for y in x] for x in n_faces]\n",
    "    sys.stdout = sys.__stdout__\n",
    "    for i in range(1,len(emb)): # frame actual\n",
    "        for j in range(len(emb[i])): # frame anteriorx  \n",
    "            s = 0.4\n",
    "            p = 0\n",
    "            for k in range(len(emb[i-1])): # cara actual\n",
    "                if is_match(i,j,i-1,k,False,n_faces,emb) < s:\n",
    "                    s = is_match(i,j,i-1,k,False,n_faces,emb)\n",
    "                    p = k\n",
    "            n_faces[i][j],n_faces[i][p] = n_faces[i][p],n_faces[i][j] \n",
    "            emb[i][j], emb[i][p] = emb[i][p], emb[i][j]\n",
    "    \n",
    "    return n_faces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "2a539f53-eccc-4eaf-ac10-d3202fea88ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediccion(path):\n",
    "    arr = nueva(path)\n",
    "    a = {}\n",
    "    for i in range(len(arr[0])):\n",
    "        temp = list(map(lambda x: x[i] , arr))\n",
    "        img_path = str(i)+path.split(\".\")[0]+\".jpg\"\n",
    "        cv2.imwrite(\"static/uploads/\"+img_path, cv2.cvtColor(temp[0], cv2.COLOR_RGB2BGR))\n",
    "        temp = np.array(temp)\n",
    "        mapa = feature_extractor.predict(temp)\n",
    "\n",
    "        cnnrnn = tf.keras.models.load_model(\"static/deepfake_rnn_3.h5\")\n",
    "        prediccion = cnnrnn.predict(mapa[None,:,:])\n",
    "        a[img_path] = \"El rostro es :{} \\n Con una probabilidad de: {:.2%}\".format(LABELS[np.argmax(prediccion)], np.amax(prediccion) )\n",
    "    return a\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4316e771-102f-46ca-9968-b234d4a1a0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALLOWED_EXTENSION = set(['txt', 'pdf', 'png', 'jpg', 'jpeg', 'gif', 'mp4'])\n",
    "IMAGE_WIDTH = 224\n",
    "IMAGE_HEIGHT = 224\n",
    "IMAGE_CHANNELS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fcc4b47-dad5-41a7-9137-88e39b10e836",
   "metadata": {},
   "outputs": [],
   "source": [
    "def allowed_file(filename):\n",
    "    return '.' in filename and \\\n",
    "     filename.rsplit('.',1)[1] in ALLOWED_EXTENSION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f4e21db2-3ee2-43e0-ac7c-f69bf45624d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on all addresses.\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      " * Running on http://192.168.1.218:5000/ (Press CTRL+C to quit)\n",
      "192.168.1.218 - - [06/Jul/2022 13:44:08] \"GET / HTTP/1.1\" 200 -\n",
      "192.168.1.218 - - [06/Jul/2022 13:44:08] \"GET /static/css/style.css HTTP/1.1\" 200 -\n",
      "192.168.1.218 - - [06/Jul/2022 13:44:08] \"GET /favicon.ico HTTP/1.1\" 404 -\n",
      "192.168.1.100 - - [06/Jul/2022 13:49:23] \"GET / HTTP/1.1\" 200 -\n",
      "192.168.1.100 - - [06/Jul/2022 13:49:23] \"GET /static/css/style.css HTTP/1.1\" 200 -\n",
      "192.168.1.100 - - [06/Jul/2022 13:49:24] \"GET /favicon.ico HTTP/1.1\" 404 -\n",
      "192.168.1.100 - - [06/Jul/2022 13:59:41] \"GET / HTTP/1.1\" 200 -\n",
      "192.168.1.100 - - [06/Jul/2022 13:59:41] \"GET /static/css/style.css HTTP/1.1\" 200 -\n",
      "192.168.1.218 - - [06/Jul/2022 14:03:51] \"GET / HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "UPLOAD_FOLDER = 'static/uploads/'\n",
    "\n",
    "app = Flask(__name__)\n",
    "app.secret_key = \"secret key\"\n",
    "app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER\n",
    "app.config['MAX_CONTENT_LENGTH'] = 16 * 1024 * 1024\n",
    "\n",
    "@app.route('/')\n",
    "def upload_form():\n",
    "    return render_template('upload.html', prediction={})\n",
    "\n",
    "@app.route('/', methods=['POST'])\n",
    "def upload_video():\n",
    "    prediction={}\n",
    "    if 'file' not in request.files:\n",
    "        flash('No file part')\n",
    "        return redirect(request.url)\n",
    "    file = request.files['file']\n",
    "    if file.filename == '':\n",
    "        flash('No image selected for uploading')\n",
    "        return redirect(request.url)\n",
    "    else:\n",
    "        filename = secure_filename(file.filename)\n",
    "        file.save(os.path.join(app.config['UPLOAD_FOLDER'], filename))\n",
    "        #print('upload_video filename: ' + filename)\n",
    "        flash('Video successfully uploaded and displayed below')\n",
    "        pred = prediccion(file.filename)\n",
    "        return render_template('upload.html', filename=filename, prediction=pred)\n",
    "    \n",
    "    \n",
    "def shutdown_server():\n",
    "    func = request.environ.get('werkzeug.server.shutdown')\n",
    "    if func is None:\n",
    "        raise RuntimeError('Not running with the Werkzeug Server')\n",
    "    func()\n",
    "\n",
    "\n",
    "@app.route('/shutdown')\n",
    "def shutdown():\n",
    "    shutdown_server()\n",
    "    return 'Server shutting down...'\n",
    "\n",
    "@app.route('/display/<filename>')\n",
    "def display_video(filename):\n",
    "    #print('display_video filename: ' + filename)\n",
    "    return redirect(url_for('static', filename='uploads/' + filename), code=301)\n",
    "\n",
    "@app.route('/display/<filename>')\n",
    "def display_image(filename):\n",
    "    #print('display_video filename: ' + filename)\n",
    "    return redirect(url_for('static', filename='uploads/' + filename), code=301)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
